{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Unsupervised Learning: Multi TS Features Embedding.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"6jZvgx5BSQ5b","colab_type":"text"},"source":["# Virtual Machine Workload Characterization  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"ulxMHztRopoo","colab_type":"text"},"source":["## VMware and virtualization\n","\n","_\"We believe that software has the power to unlock new possibilities for people and our planet. Our software forms a digital foundation that powers the apps, services, and experiences transforming the world.\"_\n","\n","VMware is an IT company – leader in the server virtualization market. Virtualization is the process of running multiple instances of different computer system in a software layer abstracted from the actual hardware. Most commonly, it refers to running multiple operating systems on a single physical server simultaneously. To the applications running on top of the virtualized machine are not aware that they run on a virtual machine."]},{"cell_type":"markdown","metadata":{"id":"HnAizJnTocN4","colab_type":"text"},"source":["## Problem description\n","\n","As two of the most-mature products, vSphere and vCenter Server provides great deal of customization and flexibility. However, given the complexity of the modern virtualization and the numerous different technologies involved in computing network and storage the customization options for VMware products grew infeasible large for common system administrators to grasp.\n","\n","At the same time different workloads have different needs. There always is some tradeoff between different functionalities of the system (like throughput and latency or  consolidation and redundancy) and there is not one configuration to serve equally well all kind of workloads. Thus, understanding the purpose of the environment is crucial. \n","\n","And while it is easy to profile and optimize single virtual machine, vSphere stack hosts millions virtual environments. In order to approach their needs proactively we have to find a data driven way to classify them.  \n","</br> \n","\n","### The Challenge \n","\n","vSphere stack enables (with the explicit agreement of the owners of the environment) the collection of on demand low level performance telemetry. Based on this data we need to identify groups of virtual machines similar with respect to different properties, such as scale, utilization pattern etc. However, we will diverge from the standard clustering algorithms (although we will use one as supporting tool) and try to achieve this through embeddings.  \n","\n","### But what is an embedding? \n","\n","Embedding is a representation of high dimensional vector on low dimensional space. Ideally this representation preserves as much information from the original vector by positioning similar inputs close together on the embedding space.\n","\n","</br> \n","### The Dataset \n","\n","The dataset consists of two main data sources related to the performance and the virtual hardware of the virtual machines (VMs).\n","\n","The **performance telemetry** is organized in a python list, containing multiple python dictionaries. Each dictionary accounts for the data of single VM. The key of the dictionary is the respective ID of the VM, and the value is pandas data frame indexed by the timestamp and containing the actual measurements of each feature.  \n","\n","</br>\n","\n","Variable Name |Df index| type|unit | Description|\n","--- | --- |--- | --- |---\n","ts  |yes|timestamp|time|Time stamp (yyyy-mm-dd HH:MM:SS) of the observation|\n","cpu_run  |no|numeric|milliseconds|The time the virtual machine use the CPU|\n","cpu_ready|no|numeric|milliseconds|The time the virtual machine wants to run a program on the CPU but waited to be scheduled|\n","mem_active|no|numeric|kiloBytes|The amount of memory actively used by the vm|\n","mem_activewrite|no|numeric|kiloBytes|Amount of memory actively being written to by the virtual machine.|\n","net_packetsRx|no|numeric|count|Number of packets received during the interval.|\n","net_packetsTx|np|numeric|count|Number of packets transmitted during the interval.|\n","\n","</br> \n","The **virtual hardware dataset** is a normal rectangular data frame indexed by the id of the VM. It represents “static” features that account basically for the scale of the system.  \n","</br>  \n","  \n","\n","Variable Name |Df index| type|unit | Description|\n","--- | --- |--- | --- |---\n","id  |yes|integer| |Unique identifier of the virtual machine |\n","memory_mb|no|integer|megabytes|Configured virtual RAM|\n","num_vcpus|no|integer|count|Number of virtual processor cores|\n","number_of_nics|no|integer|count|Number of network interface cards|\n","num_virtual_disks|no|integer|count|Number of the configured hdd|\n","os_fam|no|categorical|indetity|The operating system of the VM|\n","\n","</br></br>\n"]},{"cell_type":"markdown","metadata":{"id":"i1GcH-SpsAsE","colab_type":"text"},"source":["## Environment setup"]},{"cell_type":"markdown","metadata":{"id":"U5ncdfNUa8LZ","colab_type":"text"},"source":["### Before we begin, we should do some groundwork:\n","\n","*  Doing the initial imports\n","*  Mounting Google Drive as our file system\n","*  Setting some path variables and creating working directory \n","*  Download the data from external repo (another location in Google Drive)  \n","*  Define some functions that we will reuse many times\n","   "]},{"cell_type":"code","metadata":{"id":"LVcZKgQbUQS_","colab_type":"code","colab":{}},"source":["## SETUP THE ENVIRONMENT \n","\n","## Import general system modules\n","from google.colab import drive\n","from google_drive_downloader import GoogleDriveDownloader\n","from itertools import product\n","from itertools import chain\n","import random \n","import pickle\n","import gzip\n","import sys\n","import os\n","\n","## Core modules\n","import pandas as pd\n","import numpy as np\n","\n","## Import plotting libraries\n","from matplotlib import pyplot as plt\n","import matplotlib \n","import seaborn as sns\n","\n","## ML modules\n","from sklearn.manifold import TSNE\n","from sklearn.preprocessing import MinMaxScaler\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.decomposition import PCA \n","from sklearn.cluster import DBSCAN\n","import umap\n","\n","## Plot functions display options \n","%matplotlib inline\n","sns.set(rc={'figure.figsize':(11.7,8.27)})"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Zxg4BpxoORm_","colab_type":"code","colab":{}},"source":["## Mount your google drive as file system\n","## You should allow this notebook to access your drive\n","## ultimately receiving temporary token\n","drive.mount('/content/drive')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"DhijkR7USakl","colab_type":"code","colab":{}},"source":["## Setup our root dir\n","gdrive_root = \"/content/drive/My Drive/AMLD VMWARE/\" \n","performance_file_name = 'performance_telemetry.gzip' \n","viritual_hardware_file_name = 'virtual_hardware.csv'\n","utilities_file = 'workshop_utilities.py'\n","\n","## Construct the file path of the file we are going to download and use: \n","performance_data_full_path = gdrive_root + performance_file_name\n","virtual_hardware_data_full_path = gdrive_root + viritual_hardware_file_name\n","utilities_file_full_path = gdrive_root + utilities_file\n","\n","\n","## Check if the default working directory exist and if no, create it: \n","if not(os.path.exists(gdrive_root)):\n","    os.mkdir(gdrive_root)\n","    print(gdrive_root,' directory has been created!')\n","\n","## Add the new path to the runtime\n","## From here we will import some custom functions\n","sys.path.append(gdrive_root)\n","\n","## Set the root dir as default\n","os.chdir(gdrive_root)\n","print('File paths have been set!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"REjrY8ZIEIX8","colab_type":"code","colab":{}},"source":["## This chunk downloads data from our shared storage and makes a \"local copy to your personal google drive\"\n","## After the first execution of this chunk the data is cached and in the next run it won’t download the data. \n","## In order to execute it again, you have to delete the files from the folder (or the entire folder) \n","\n","\n","## Files location\n","performance_tm_fileid = '1tQulULMw09XQ5rWKF0fqEzXHRwX2CwBS'\n","virtual_hw_fileid = \"136ZxWcIflPD_yol5Nhkj9LVeYB9-1W6L\"\n","workshop_util_file = '1amC8ZhG7ZQt1fwJoidLC8GF58yXTQrJz'\n","\n","## Download Data\n","GoogleDriveDownloader.download_file_from_google_drive(file_id = performance_tm_fileid , dest_path = performance_data_full_path     )\n","GoogleDriveDownloader.download_file_from_google_drive(file_id = virtual_hw_fileid     , dest_path = virtual_hardware_data_full_path)\n","GoogleDriveDownloader.download_file_from_google_drive(file_id = workshop_util_file    , dest_path = utilities_file_full_path       )\n","\n","### Print the content of our working folder\n","#it should contain ['performance_telemetry.gzip', 'virtual_hardware.csv', 'workshop_utilities.py']\n","print('Dir content:', os.listdir(gdrive_root))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2-36tQ9XJ6Vs","colab_type":"code","colab":{}},"source":["## Import some predefined utility functions\n","from workshop_utilities import truncate_by_quantile, plot_single_vm, sample_observation, model_params_product, model_train, plot_results, plot_embedding\n","print('Custom utility functions loaded!')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1GHDPJhR_sq","colab_type":"code","colab":{}},"source":["## Load data in memory for this session: \n","\n","## Load performance data\n","with gzip.open(performance_file_name, 'rb') as fconn:\n","    perf_data = pickle.load(fconn)\n","    print('Performance Telemetry successfully loaded !')\n","\n","## Load virtual hardware\n","virt_hw = pd.read_csv(viritual_hardware_file_name, sep = ';', index_col = 'id')\n","print('Virtual Hardware successfully loaded !')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"i8qD5zkLjOz7","colab_type":"code","colab":{}},"source":["## Define function that applies numeric transformation \n","def return_transformer(transformer = None, numeric_constant = 1, lambd = 0):\n","    if transformer == None or transformer == 'None':\n","        return(lambda x: x)\n","    elif transformer == 'ln' or (transformer=='boxcox' and lambd == 0):\n","        return(lambda x: np.log(x+numeric_constant))\n","    elif transformer == 'log10':\n","        return(lambda x: np.log10(x+numeric_constant))\n","    elif transformer=='boxcox':\n","        return(lambda x: (np.power(x+numeric_constant, lambd) - 1)/lambd ) \n","\n","    ### ADD YOUR OWN TRANSFORMATION HERE "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"rCTP555cChf9","colab_type":"code","colab":{}},"source":["### To run all code chunks go to Runtime>Run before (Ctrl + F8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QZMzsCEpB_bF","colab_type":"text"},"source":["### Env setup end"]},{"cell_type":"markdown","metadata":{"id":"qbQukxBlqIId","colab_type":"text"},"source":["## First look at our data"]},{"cell_type":"code","metadata":{"id":"GZBbwfmAGaKQ","colab_type":"code","colab":{}},"source":["## Very basic data exploration \n","## Define some global variables that we will reuse \n","##   N: Number of virtual machines \n","##   D: Number of performance features \n","##   T: Temporal dim: number of time samples\n","##   F: List of features \n","## VMs: List with all ids of the virtual machines\n","\n","N = len(perf_data)\n","VMs = list(perf_data.keys())\n","T , D = perf_data[VMs[0]].shape\n","F = list(perf_data[VMs[0]].columns)\n","\n","\n","print('Dictionary with', N ,' virtual machines!')\n","print('Each VM has ',T, ' observations in time!')\n","print('There are',D, 'performance features:',  F)\n","print('\\n First rows of the first virtual machine \\n')\n","pd.set_option('display.max_columns', 10)\n","print(perf_data[1].head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"zpKG3HJDnn3n","colab_type":"code","colab":{}},"source":["##   D_hw: Number of performance features \n","##   F_hw: List of features  \n","\n","_ , D_hw = virt_hw.shape\n","F_hw = list(virt_hw.columns)\n","\n","print('Dictionary with', N ,' virtual machines!')\n","print('There are',D_hw, 'virtual hardware features:',  F_hw, '\\n')\n","\n","print('Missing values:')\n","print(np.sum(virt_hw.isna()))\n","\n","print('\\nData types:')\n","print(virt_hw.dtypes)\n","\n","print('\\nData Frame Summary:')\n","virt_hw.describe()\n","\n","print(virt_hw.head())"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"VYdo9Z5Ypfr8","colab_type":"code","colab":{}},"source":["fig, axes = plt.subplots(nrows = 2, ncols=3, sharex = False, sharey = False, figsize = (40, 10))\n","\n","\n","for i, metric in enumerate(F_hw):\n","    \n","    ax = axes.reshape(-1)[i]\n","    data = virt_hw.groupby([metric])[metric].count()\n","    x_ax = [str(i) for i in data.index.values]\n","    ax.bar(x_ax, height  = data.values)\n","    ax.set_title(metric)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"032ZIfb2TGtX","colab_type":"text"},"source":["### Inspect single VM series"]},{"cell_type":"code","metadata":{"id":"XIc_Tq9-jDbz","colab_type":"code","colab":{}},"source":["## \n","DATA = perf_data\n","VMID = None ## None for random VM \n","TRANSFORMATION = return_transformer(None)\n","\n","\n","###\n","plot_single_vm(__perf_data = DATA\n","             , __vmid = VMID\n","             , transformation = TRANSFORMATION\n","             )"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"saMQ1hIcFNYy","colab_type":"text"},"source":["### Note on data transformation \n","\n","Transforming variable means replacing the actual value x with some function of that variable f(x). As a result, we change the distribution and the relationship of this variable in some beneficial way. \n","\n","Here we will consider two basic techniques:\n","*  Box-Cox(power) transformation - This is a useful data transformation technique used to stabilize variance, make the data more normal distribution-like, improve the validity of measures of association.\n","\n","*  Logarithmic transformation - logarithmic function has many nice properties that are commonly exploited in the analysis of time series. Change of natural log roughly equals percentage change. It also converts multiplicative relationship into additive (log(XY) = log(X) + log(Y)).  Natural log transformation is a private case of Box-Cox transformation\n","  \n","\n","*  Data truncation - many of the statistics, that we are going to compute are quite sensitive to extreme values and outliers. Thus, it might be worth sacrificing tiny portion of the data in order to limit the effect of extreme values on our estimates. Note that such transformation might obscure the actual distribution and should be applied with care."]},{"cell_type":"code","metadata":{"id":"8cxUIrYub7-c","colab_type":"code","colab":{}},"source":["################\n","## PARAMETERS ##\n","################\n","\n","VMID = None                # None or integer between 1 and 707 \n","METRIC = 'mem_activewrite' # If None then random metric is chosen\n","TRUNCATE_VARIABLE = True # \n","TRUNC_QRANGE = [0,0.99]     # no truncation =[0,1]\n","TRANSFORMATION = return_transformer(None) ## returns lambda expression. Arguments:None, 'ln', log10,  'boxcox' + lambda\n","\n","\n","##############\n","if METRIC == None:\n","    METRIC = F[np.random.randint(0, D)]\n","\n","VMID, data = sample_observation(perf_data, vmid = VMID)\n","\n","\n","## Inspect the distribution of the variable: \n","fig = plt.figure(figsize=(20,10))\n","gs0 = fig.add_gridspec(2,1)\n","gs1 = gs0[1].subgridspec(1,2)\n","\n","\n","ax0 = fig.add_subplot(gs0[0])\n","ax1 = fig.add_subplot(gs1[0])\n","ax2 = fig.add_subplot(gs1[1])\n","ax1.set_title('Untransformed distribution')\n","ax2.set_title('Transformed distribution')\n","\n","ax0.set_title('Series in time {feat} for VMID {vmid}:'.format(feat = METRIC, vmid = VMID))\n","ax0.plot(data.index,  data[METRIC], linewidth = 0.9)\n","ax0_0 = ax0.twinx()\n","ax0_0.plot(data.index, TRANSFORMATION(data[METRIC]), label = 'transformed', c = 'orange', linewidth = 0.5)\n","## First plot the original series and then truncate the data \n","\n","data['truncated'] = data[METRIC]\n","if TRUNCATE_VARIABLE:\n","    #data = data.apply(lambda x: truncate_by_quantile(x, trunc_range = TRUNC_QRANGE))\n","    data['truncated'] = truncate_by_quantile(data[METRIC], trunc_range = TRUNC_QRANGE)\n","\n","sns.distplot(data[METRIC].dropna(), kde = False, ax = ax1, bins = 40)\n","sns.distplot(TRANSFORMATION(data['truncated'].dropna()), kde = False, ax = ax2, bins = 40 )\n","\n","ax1.set_xlabel(None)\n","ax2.set_xlabel(None)\n","plt.legend()\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GGFSQJZYXZn1","colab_type":"text"},"source":["### Which transformation? \n","Among thigs to consider when you transform your variables are what works with the data, does it makes sense and is there a natural interpretation after transforming the data?  \n","</br>\n","\n","**References:**  \n","[Transformations: an introduction with STATA](http://fmwww.bc.edu/repec/bocode/t/transint.html)  \n","[The logarithm transformation](https://people.duke.edu/~rnau/411log.htm)  \n","[MAKING DATA NORMAL USING BOX-COX POWER TRANSFORMATION](https://www.isixsigma.com/tools-templates/normality/making-data-normal-using-box-cox-power-transformation/)  \n","[Yeo-Johnson Power Transformation](https://www.stat.umn.edu/arc/yjpower.pdf)\n","\n","\n","</br> \n","**Some insights regarding the performance telemetry:**\n","\n","*  Our data is strictly non-negative (convenient for power transformations)  \n","*  Most of the performance metrics do not exhibit clear trend\n","*  Missing data due do data collection routine \n","*  There are instances with multimodal distribution \n","\n"]},{"cell_type":"markdown","metadata":{"id":"n7Y3EFFeKypF","colab_type":"text"},"source":["## Feature engineering: Basic descriptives\n","\n","\n","The first stage of our FE is to obtain very basic descriptive statistics. Descriptive statistics are coefficients that summarize some aspect of the empirical distribution of a variable. The most common aspects that are measured by the descriptive statistics are the central tendency (mean, median, mode) and variability (standard deviation, variance, kurtosis and skewness).\n","<br></br>\n","**_PROS:_**   \n","*  The descriptive are not as sensitive to relatively small amounts of missing data   \n","*  Easy and fast to compute and often comes already implemented in the most software packages   \n","*  Easy to interpret   \n","\n","\n"," **_CONS:_**   \n","*  They are not very expressive and can hide a lot of the information  \n","*  Some of the basic descriptives are very sensitive to extreme values and need some preprocessing\n","<br></br>\n","\n","In order to mitigate the effect of extreme values we will do some statistical tricks, including truncating all values past a threshold and transforming our data on another scale. This way we will improve the statistical properties of our data and make our estimates more robust."]},{"cell_type":"markdown","metadata":{"id":"z8f7DSLPkmdE","colab_type":"text"},"source":["### How will we create our initial features?  \n","\n","1.  Apply numeric transformation that will change the scale and the distribution of the initial data\n","2. Apply multiple function that takes one or more pd.Series as argument and yields single numerical representation of this / these series. Store them in some data structure  \n","3. Flatten this data structure into vector  for each virtual machine  \n","\n","Ultimately, we are going to end with tidy rectangular dataset where each row represents single virtual machine and each column represents feature that we have extracted from our original data.  \n","\n","[Pandas Series Methods](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Series.html)"]},{"cell_type":"code","metadata":{"id":"csfSpQah1IPw","colab_type":"code","colab":{}},"source":["## How can we apply custom functions ? \n","## Define function that operates on pd Series\n","\n","def missing_count(series):\n","    return(len(series) - series.count())\n","\n","def Q25(series):\n","    return(series.quantile(0.25))\n","\n","def Q50(series):\n","    return(series.quantile(0.50))\n","\n","def Q75(series):\n","    return(series.quantile(0.75))\n","\n","## ADD YOUR CUSTOM FUNCTIONS HERE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmOp77AXnm_w","colab_type":"code","colab":{}},"source":["## Single case example: \n","\n","## Apply multiple build in aggregation functions with relatively fast implementation \n","## the available functions are sum, mean, min, max, var, sem, skew, kurt, count \n","## sem - standard error of the mean\n","## kurt - kurtosis (4th moment)\n","## skew - skewness (3rd moment)\n","## var  - variance (2nd moment)\n","## These functions can be passed as an array to each column of the data frame: \n","\n","_, data  = sample_observation(perf_data)\n","np.round(data.agg(['mean', 'min', 'max', 'var', 'skew', Q50,missing_count]), 3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"4IcARgZd0q1T","colab_type":"code","colab":{}},"source":["## Correlation matrix\n","data.corr()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ny3bQ3xVytLO","colab_type":"text"},"source":["#### Creating the features\n","</br> \n","By the end of our feature engineering section we will end up with single data frame that combines the information from both descriptive statistics and correlation data.  \n","*  Our initial datasets will be called  **descriptive_statistics** and **correlation_data**. There is a pandas view that consolidates them into single data frame called **non_temporal_features**  \n","*  The features in this data set can be referenced by the column names\n","  * The names of the features, yielded from the descriptive statistics analysis are stored in **descriptive_statistics_columns** variable\n","  * The names of the features, yielded from the correlation matrix are stored in **correlation_features_columns** variable  \n"]},{"cell_type":"code","metadata":{"id":"_hLQ_aaMd-oX","colab_type":"code","colab":{}},"source":["## FEATURE GENERATION CHUNK ##\n","## FOR EACH VM\n","##     FOR EACH METRIC [cpu_run, cpu_ready ... net_packetsRx]\n","##          1. TRUNCATE BY QUANTILE \n","##          2. APPLY TRANSFORMATION\n","##          3. COMPUTE EACH METRIC IN GENERAL_STATISTICS \n","##          5. CREATE FEATURE VECTOR WITH COLUMNS {METRIC}_{STATISTIC} [cpu_run_mean, cpu_run_std, .... ]\n","##          4. COMPUTE CORRELATION \n","##          5. FOR EACH UNIQUE PAIR OF METRICS CREATE FEATURE VECTOR [corr_cpu_run_cpu_ready, corr_cpu_run_mem_active, ... ]\n","## 6. COMBINE THE INFORMATION FOR EACH VIRTUAL MACHINE IN TWO DATA FRAMES   \n","\n","\n","\n","GENERAL_STATISTICS = ['mean','std','skew',Q50, Q25, Q75]\n","TRANSFORMATION =  return_transformer('None')\n","TRUNCATE_VARIABLE = True\n","TRUNC_QRANGE = [0.001, 0.999]\n","\n","\n","\n","\n","#######################\n","## INIT PLACEHOLDERS ##\n","#######################\n","\n","descriptive_statistics = []\n","correlation_data = []\n","i = 1\n","for k, data in perf_data.items():\n","\n","    ###################\n","    ## TRUNCATE DATA ##\n","    ###################\n","\n","    if TRUNCATE_VARIABLE:\n","        data = data.apply(lambda x: truncate_by_quantile(x, trunc_range = TRUNC_QRANGE))\n","    \n","    ## Apply transformation\n","    data = TRANSFORMATION(data)\n","\n","    ############################\n","    ## Descriptive statistics ##\n","    ############################\n","\n","    ## Apply aggregation functions\n","    general_descriptives = data.agg(GENERAL_STATISTICS)\n","    \n","    ## apply modifications to general descriptives: \n","    ## 1. converts the data frame to series with index [(summary_statistic,performance_metric)]\n","    general_descriptives = general_descriptives.stack()\n","\n","    ## Convert 2d index to 1d by string concat: \n","    general_descriptives.index = general_descriptives.index.map('{0[1]}_{0[0]}'.format)\n","\n","    ## Convert the series to data frame and transpose it \n","    general_descriptives = general_descriptives.to_frame().T\n","\n","    ## Add the VM indentifier: \n","    general_descriptives['vmid'] = k\n","\n","    ########################\n","    ## Correlation matrix ##\n","    ########################\n","\n","    # Calculate correlation matrix:  \n","    correlation_matrix = data.corr()\n","    corr_df_tmp = pd.DataFrame(index=[k])\n","\n","\n","    ## Iterate over the elements of the (cross) correlation matrix\n","    ## Take the elements below the diagonals \n","    ## This way we flatten the correlation matrix into feature vector: \n","    \n","    for mrow in range(D):\n","        for mcol in range(D):\n","            if mrow >= mcol:\n","                continue\n","\n","            # Construct new col name:\n","            new_col = 'corr_' + F[mrow] + '_' + F[mcol]\n","            corr_df_tmp[new_col] = correlation_matrix.iloc[mrow,mcol]\n","    \n","    if i%100 == 0:\n","        print('Finished iteration', i, 'out of', N)\n","    i += 1\n","    \n","    ## Finally set vmid as index (on the fly)\n","    ## Join correlation data with descriptive statistics:\n","    ## as add the record to the data placeholder:\n","\n","    descriptive_statistics.append(general_descriptives.set_index('vmid'))\n","    correlation_data.append(corr_df_tmp)\n","\n","print('Finished iteration', N, ' out of ', N)\n","\n","## The loop left us with list of single row data frames both for descriptive statistics and correlation matrix\n","## Next step is to combine the whole data into one big data frame  \n","descriptive_statistics = pd.concat(descriptive_statistics)\n","correlation_data = pd.concat(correlation_data)\n","\n","## We will further process the different families of features \n","## Thats why we keep their names in separate variables \n","descriptive_statistics_columns = descriptive_statistics.columns\n","correlation_features_columns = correlation_data.columns\n","\n","## Finally merge the final dataset into one \n","non_temporal_features = descriptive_statistics.merge(correlation_data, left_index = True, right_index= True)\n","non_temporal_features_index = non_temporal_features.index\n","\n","## Small trick to ensure alignment between our features and the virtual hardware data\n","virt_hw = virt_hw.loc[non_temporal_features.index]"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LySDp_OJXZs6","colab_type":"text"},"source":["#### Inspect the features\n","Visualize the distributions of the features that we’ve created. Please keep in mind the transformation that we’ve applied over the original series."]},{"cell_type":"code","metadata":{"id":"VFpXrhGlG04V","colab_type":"code","outputId":"25798c28-b428-4ff0-fa46-8c321690ff1a","executionInfo":{"status":"ok","timestamp":1579772675250,"user_tz":-120,"elapsed":661,"user":{"displayName":"AMLD TemUserMaster","photoUrl":"","userId":"00430682455941763130"}},"colab":{"base_uri":"https://localhost:8080/","height":297}},"source":["## Inspect the summary statistics of the correlation features\n","pd.set_option('display.max_columns', 50)\n","np.round(correlation_data.describe(), 2)\n"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>corr_cpu_run_cpu_ready</th>\n","      <th>corr_cpu_run_mem_active</th>\n","      <th>corr_cpu_run_mem_activewrite</th>\n","      <th>corr_cpu_run_net_packetsRx</th>\n","      <th>corr_cpu_run_net_packetsTx</th>\n","      <th>corr_cpu_ready_mem_active</th>\n","      <th>corr_cpu_ready_mem_activewrite</th>\n","      <th>corr_cpu_ready_net_packetsRx</th>\n","      <th>corr_cpu_ready_net_packetsTx</th>\n","      <th>corr_mem_active_mem_activewrite</th>\n","      <th>corr_mem_active_net_packetsRx</th>\n","      <th>corr_mem_active_net_packetsTx</th>\n","      <th>corr_mem_activewrite_net_packetsRx</th>\n","      <th>corr_mem_activewrite_net_packetsTx</th>\n","      <th>corr_net_packetsRx_net_packetsTx</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>count</th>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","      <td>707.00</td>\n","    </tr>\n","    <tr>\n","      <th>mean</th>\n","      <td>0.39</td>\n","      <td>0.21</td>\n","      <td>0.21</td>\n","      <td>0.33</td>\n","      <td>0.35</td>\n","      <td>0.06</td>\n","      <td>0.07</td>\n","      <td>0.22</td>\n","      <td>0.16</td>\n","      <td>0.62</td>\n","      <td>0.10</td>\n","      <td>0.14</td>\n","      <td>0.11</td>\n","      <td>0.14</td>\n","      <td>0.67</td>\n","    </tr>\n","    <tr>\n","      <th>std</th>\n","      <td>0.42</td>\n","      <td>0.27</td>\n","      <td>0.26</td>\n","      <td>0.36</td>\n","      <td>0.36</td>\n","      <td>0.19</td>\n","      <td>0.19</td>\n","      <td>0.34</td>\n","      <td>0.31</td>\n","      <td>0.18</td>\n","      <td>0.20</td>\n","      <td>0.22</td>\n","      <td>0.20</td>\n","      <td>0.21</td>\n","      <td>0.36</td>\n","    </tr>\n","    <tr>\n","      <th>min</th>\n","      <td>-0.98</td>\n","      <td>-0.62</td>\n","      <td>-0.58</td>\n","      <td>-0.80</td>\n","      <td>-0.65</td>\n","      <td>-0.87</td>\n","      <td>-0.87</td>\n","      <td>-0.77</td>\n","      <td>-0.79</td>\n","      <td>0.15</td>\n","      <td>-0.36</td>\n","      <td>-0.35</td>\n","      <td>-0.39</td>\n","      <td>-0.21</td>\n","      <td>-0.31</td>\n","    </tr>\n","    <tr>\n","      <th>25%</th>\n","      <td>0.12</td>\n","      <td>0.03</td>\n","      <td>0.03</td>\n","      <td>0.07</td>\n","      <td>0.09</td>\n","      <td>-0.02</td>\n","      <td>-0.01</td>\n","      <td>-0.00</td>\n","      <td>-0.02</td>\n","      <td>0.50</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.00</td>\n","      <td>0.01</td>\n","      <td>0.34</td>\n","    </tr>\n","    <tr>\n","      <th>50%</th>\n","      <td>0.41</td>\n","      <td>0.12</td>\n","      <td>0.13</td>\n","      <td>0.22</td>\n","      <td>0.28</td>\n","      <td>0.02</td>\n","      <td>0.03</td>\n","      <td>0.09</td>\n","      <td>0.05</td>\n","      <td>0.58</td>\n","      <td>0.04</td>\n","      <td>0.06</td>\n","      <td>0.04</td>\n","      <td>0.07</td>\n","      <td>0.83</td>\n","    </tr>\n","    <tr>\n","      <th>75%</th>\n","      <td>0.75</td>\n","      <td>0.38</td>\n","      <td>0.34</td>\n","      <td>0.56</td>\n","      <td>0.59</td>\n","      <td>0.08</td>\n","      <td>0.11</td>\n","      <td>0.38</td>\n","      <td>0.25</td>\n","      <td>0.73</td>\n","      <td>0.13</td>\n","      <td>0.20</td>\n","      <td>0.15</td>\n","      <td>0.19</td>\n","      <td>0.97</td>\n","    </tr>\n","    <tr>\n","      <th>max</th>\n","      <td>1.00</td>\n","      <td>0.96</td>\n","      <td>0.96</td>\n","      <td>1.00</td>\n","      <td>1.00</td>\n","      <td>0.95</td>\n","      <td>0.95</td>\n","      <td>0.99</td>\n","      <td>0.97</td>\n","      <td>1.00</td>\n","      <td>0.89</td>\n","      <td>0.91</td>\n","      <td>0.93</td>\n","      <td>0.92</td>\n","      <td>1.00</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["       corr_cpu_run_cpu_ready  corr_cpu_run_mem_active  \\\n","count                  707.00                   707.00   \n","mean                     0.39                     0.21   \n","std                      0.42                     0.27   \n","min                     -0.98                    -0.62   \n","25%                      0.12                     0.03   \n","50%                      0.41                     0.12   \n","75%                      0.75                     0.38   \n","max                      1.00                     0.96   \n","\n","       corr_cpu_run_mem_activewrite  corr_cpu_run_net_packetsRx  \\\n","count                        707.00                      707.00   \n","mean                           0.21                        0.33   \n","std                            0.26                        0.36   \n","min                           -0.58                       -0.80   \n","25%                            0.03                        0.07   \n","50%                            0.13                        0.22   \n","75%                            0.34                        0.56   \n","max                            0.96                        1.00   \n","\n","       corr_cpu_run_net_packetsTx  corr_cpu_ready_mem_active  \\\n","count                      707.00                     707.00   \n","mean                         0.35                       0.06   \n","std                          0.36                       0.19   \n","min                         -0.65                      -0.87   \n","25%                          0.09                      -0.02   \n","50%                          0.28                       0.02   \n","75%                          0.59                       0.08   \n","max                          1.00                       0.95   \n","\n","       corr_cpu_ready_mem_activewrite  corr_cpu_ready_net_packetsRx  \\\n","count                          707.00                        707.00   \n","mean                             0.07                          0.22   \n","std                              0.19                          0.34   \n","min                             -0.87                         -0.77   \n","25%                             -0.01                         -0.00   \n","50%                              0.03                          0.09   \n","75%                              0.11                          0.38   \n","max                              0.95                          0.99   \n","\n","       corr_cpu_ready_net_packetsTx  corr_mem_active_mem_activewrite  \\\n","count                        707.00                           707.00   \n","mean                           0.16                             0.62   \n","std                            0.31                             0.18   \n","min                           -0.79                             0.15   \n","25%                           -0.02                             0.50   \n","50%                            0.05                             0.58   \n","75%                            0.25                             0.73   \n","max                            0.97                             1.00   \n","\n","       corr_mem_active_net_packetsRx  corr_mem_active_net_packetsTx  \\\n","count                         707.00                         707.00   \n","mean                            0.10                           0.14   \n","std                             0.20                           0.22   \n","min                            -0.36                          -0.35   \n","25%                             0.00                           0.01   \n","50%                             0.04                           0.06   \n","75%                             0.13                           0.20   \n","max                             0.89                           0.91   \n","\n","       corr_mem_activewrite_net_packetsRx  corr_mem_activewrite_net_packetsTx  \\\n","count                              707.00                              707.00   \n","mean                                 0.11                                0.14   \n","std                                  0.20                                0.21   \n","min                                 -0.39                               -0.21   \n","25%                                  0.00                                0.01   \n","50%                                  0.04                                0.07   \n","75%                                  0.15                                0.19   \n","max                                  0.93                                0.92   \n","\n","       corr_net_packetsRx_net_packetsTx  \n","count                            707.00  \n","mean                               0.67  \n","std                                0.36  \n","min                               -0.31  \n","25%                                0.34  \n","50%                                0.83  \n","75%                                0.97  \n","max                                1.00  "]},"metadata":{"tags":[]},"execution_count":42}]},{"cell_type":"code","metadata":{"id":"p3U4Yy8dRnwY","colab_type":"code","colab":{}},"source":["TRANSFORMATION = return_transformer(None)\n","\n","## Inspect the distribution of cross correlation\n","g = sns.FacetGrid(pd.melt(TRANSFORMATION(correlation_data)), col = 'variable', col_wrap = 5, aspect = 1.5)\n","g.map(plt.hist, 'value', bins = 40)\n","plt.subplots_adjust(top=0.9)\n","g.fig.suptitle('CORRELATION FEATURES')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rnfIxnMfZ6A1","colab_type":"text"},"source":["\n","We can see that the distribution  of the correlation is skewed towards the positive correlations . There are several cross-correlation features with bi-modal distributions.\n"]},{"cell_type":"code","metadata":{"id":"UzarNt89Z1Fn","colab_type":"code","colab":{}},"source":["## Summary of the descriptive statistics features\n","pd.set_option('display.max_columns', 50)\n","print('Min feature value is:', np.min(np.round(descriptive_statistics.describe(), 1).values))\n","np.round(descriptive_statistics.describe(), 1)\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"KVoAIf_NS5VP","colab_type":"code","colab":{}},"source":["## Inspect the distribution of the descriptive statistics \n","TRANSFORMATION = return_transformer('ln', numeric_constant=5)\n","\n","#######\n","## Transform the data from wide to long format \n","data = TRANSFORMATION(descriptive_statistics)\n","data = pd.melt(data)\n","#data['variable']  = TRANSFORMATION(data['variable'])\n","\n","## Plot \n","g = sns.FacetGrid(data, col = 'variable', col_wrap = 6, sharex = False, sharey=False, aspect = 1.5)\n","g.map(plt.hist, 'value', bins = 40)\n","plt.subplots_adjust(top=0.9)\n","g.fig.suptitle('DESCRIPTIVE FEATURES')"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-3K7r4qpDJcX","colab_type":"text"},"source":["#### Apply transformation\n","\n","Each of the following chunks creates a data frame where it applies transformations. We can either choose to leave the data untransformed, to standardize (subtract mean and divide by std) or to normalize data within some numeric range (typically 0, 1 or -1,1).\n","Typically, is not good idea to apply different scaling to the different features.  A rule of the thumb (at least for this workshop) is to have up to one transformation and one scaling.\n"]},{"cell_type":"markdown","metadata":{"id":"sE2O2ZGIzRLq","colab_type":"text"},"source":["##### Descriptive statistics transformations"]},{"cell_type":"code","metadata":{"id":"iPLBZ1HmAo9B","colab_type":"code","colab":{}},"source":["############################\n","##:: DATA PREPROCESSING ::##\n","##::    DESCRIPTIVES    ::##\n","############################\n","\n","## Apply transformation\n","TRANSFORMATION = return_transformer(None)\n","\n","APPLY_STANDARD_SCALING = False\n","\n","APPLY_MIN_MAX_SCALING = True\n","MIN_MAX_SCALER_RANGE = (-1, 1)\n","\n","\n","##############\n","## We would like to retain the original features\n","## Thus we create hard copy of the data and operate on it:\n","descriptive_statistics_transformed = descriptive_statistics.copy()\n","\n","## Rescale descriptive statistics data to within  MIN MAX SCALER RANGE \n","## The correlation features are naturally scaled within this range \n","min_max_scaler_instance = MinMaxScaler(feature_range = MIN_MAX_SCALER_RANGE)\n","standart_scaler_instance = StandardScaler()\n","\n","## Apply transformation\n","descriptive_statistics_transformed[descriptive_statistics_columns] = TRANSFORMATION(descriptive_statistics_transformed[descriptive_statistics_columns])\n","\n","\n","## APPLY MIN-MAX SCALING TO DESCRIPTIVE STATISTICS DATA\n","if APPLY_MIN_MAX_SCALING:\n","    descriptive_statistics_transformed[descriptive_statistics_columns] = min_max_scaler_instance.fit_transform(descriptive_statistics_transformed[descriptive_statistics_columns])\n","\n","## APPLY MIN-MAX SCALING TO DESCRIPTIVE STATISTICS DATA\n","if APPLY_STANDARD_SCALING:\n","    #modeling_data[STANDARD_SCALING_COLUMNS] = standart_scaler_instance.fit_transform(modeling_data[STANDARD_SCALING_COLUMNS])\n","    descriptive_statistics_transformed[descriptive_statistics_columns] = standart_scaler_instance.fit_transform(descriptive_statistics_transformed[descriptive_statistics_columns])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_v92PyK3rHu","colab_type":"text"},"source":["##### Correlation features transformations"]},{"cell_type":"code","metadata":{"id":"97gLOzMH8C8v","colab_type":"code","colab":{}},"source":["############################\n","##:: DATA PREPROCESSING ::##\n","##::     CORRELATION    ::##\n","############################\n","\n","## Apply transformation\n","TRANSFORMATION = return_transformer(None)\n","\n","APPLY_STANDARD_SCALING = False\n","\n","APPLY_MIN_MAX_SCALING = False\n","MIN_MAX_SCALER_RANGE = (-1, 1)\n","\n","###############\n","## We would like to retain the original features\n","## Thus we create hard copy of the data and operate on it:\n","correlation_data_transformed = correlation_data.copy()\n","\n","## Rescale descriptive statistics data to within  MIN MAX SCALER RANGE \n","## The correlation features are naturally scaled within this range \n","min_max_scaler_instance = MinMaxScaler(feature_range = MIN_MAX_SCALER_RANGE)\n","standart_scaler_instance = StandardScaler()\n","\n","## Apply transformation\n","correlation_data_transformed[correlation_features_columns] = TRANSFORMATION(correlation_data_transformed[correlation_features_columns])\n","\n","\n","## APPLY MIN-MAX SCALING TO DESCRIPTIVE STATISTICS DATA\n","if APPLY_MIN_MAX_SCALING:\n","    correlation_data_transformed[correlation_features_columns] = min_max_scaler_instance.fit_transform(correlation_data_transformed[correlation_features_columns])\n","\n","## APPLY MIN-MAX SCALING TO DESCRIPTIVE STATISTICS DATA\n","if APPLY_STANDARD_SCALING:\n","    #modeling_data[STANDARD_SCALING_COLUMNS] = standart_scaler_instance.fit_transform(modeling_data[STANDARD_SCALING_COLUMNS])\n","    correlation_data_transformed[correlation_features_columns] = standart_scaler_instance.fit_transform(correlation_data_transformed[correlation_features_columns])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UW8cmsht3yPz","colab_type":"text"},"source":["##### Virtual hardware transformations"]},{"cell_type":"code","metadata":{"id":"gQQpv5adJNik","colab_type":"code","colab":{}},"source":["###############################\n","###:: DATA PREPROCESSING ::####\n","###::VIRTUAL HARDWARE DATA::###\n","###############################\n","\n","## Apply transformation\n","TRANSFORMATION = return_transformer(None)\n","\n","APPLY_STANDARD_SCALING = False\n","\n","APPLY_MIN_MAX_SCALING = True\n","MIN_MAX_SCALER_RANGE = (-1, 1)\n","\n","\n","## Make copy of the original data\n","virt_hw_transformed = virt_hw.copy()\n","virt_hw_transformed_columns = virt_hw_transformed.columns\n","\n","min_max_scaler_instance = MinMaxScaler(feature_range = MIN_MAX_SCALER_RANGE)\n","\n","## One hot encode categorical \n","\n","virt_hw_numerics = []\n","for col in virt_hw_transformed_columns:\n","    col_type = virt_hw_transformed[col].dtypes.name\n","    \n","    if col_type == 'object' or col_type == 'category':\n","        dummy_vars = pd.get_dummies(virt_hw_transformed[col], prefix = col)\n","        virt_hw_transformed = pd.concat([virt_hw_transformed,dummy_vars],axis=1)\n","        virt_hw_transformed.drop([col],axis=1, inplace=True)\n","    else:\n","        virt_hw_transformed[col] = TRANSFORMATION(virt_hw_transformed[col])\n","        virt_hw_numerics.append(col)\n","\n","if APPLY_MIN_MAX_SCALING:\n","    min_max_scaler_instance = MinMaxScaler(feature_range = MIN_MAX_SCALER_RANGE)\n","    virt_hw_transformed[virt_hw_numerics] = min_max_scaler_instance.fit_transform(virt_hw_transformed[virt_hw_numerics])\n","\n","if APPLY_STANDARD_SCALING:\n","    standart_scaler_instance = StandardScaler()\n","    virt_hw_transformed[virt_hw_numerics] = standart_scaler_instance.fit_transform(virt_hw_transformed[virt_hw_numerics])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1SjdZQPg33Lz","colab_type":"text"},"source":["##### Assemble the dataset for modeling"]},{"cell_type":"code","metadata":{"id":"tHzuHHeQ6FKV","colab_type":"code","colab":{}},"source":["## CREATE FINAL DATASET ## \n","## CHOOSE WHICH FEATURES TO INCLUDE FOR THE FINAL DATASET\n","## BY DEFAULT, WE WILL MODEL ALL FEATURES THAT WE’VE CREATED \n","## HOWEVER, WE CAN MAKE MODELS OUTPUT EASIER TO INTERPRET IF WE USE INFORMATION ONLY FOR ONE RESOURCE TYPE (CPU or NETWORK) \n","\n","\n","## To include columns that contains cpu_run\n","## descriptive_statistics_transformed.filter(regex = 'cpu_run')\n","\n","## To exclude columns containing cpu_run string \n","## descriptive_statistics_transformed.loc[:,~descriptive_statistics_transformed.columns.str.contains('cpu_run')]\n","\n","\n","first_stage_data_list = [\n","                              descriptive_statistics_transformed\n","                             ,correlation_data_transformed\n","                             ,virt_hw_transformed\n","                            ]\n","\n","\n","\n","\n","## Init empty data frame: \n","first_stage_data = pd.DataFrame(index = non_temporal_features_index)\n","\n","## \n","for i in first_stage_data_list:\n","    #data = i.copy() ## in the cases where there will be more transformations \n","    data = i  \n","    first_stage_data = pd.merge(first_stage_data, data, left_index=True, right_index=True)\n","\n","print('Final dataset is with shape:', first_stage_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YFwKpT1NXF3a","colab_type":"code","colab":{}},"source":["### To run all code chunks go to Runtime>Run before (Ctrl + F8)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"d_KCRPjgu5dw","colab_type":"text"},"source":["## Principal component analysis\n","\n","Although PCA can be though as form of embedding by itself, despite all benefits (speed, efficiency, interpretability), it suffers from a major drawback. It is tricky to capture (by default) non-linear relationships.   \n","</br> \n","**References:**  \n","https://towardsdatascience.com/a-one-stop-shop-for-principal-component-analysis-5582fb7e0a9c  \n","http://setosa.io/ev/principal-component-analysis/  \n","[Making sense of principal component analysis, eigenvectors & eigenvalues](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues)\n","\n","\n"]},{"cell_type":"code","metadata":{"id":"d7RfJHc-E8uy","colab_type":"code","colab":{}},"source":["from sklearn.decomposition import PCA "],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5SnQIGI4u4An","colab_type":"code","colab":{}},"source":["## Plot correlation matrix\n","cmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\n","sns.clustermap(first_stage_data.corr(), figsize= (20,20), cmap = cmap)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"mNB8dte7omh_","colab_type":"code","colab":{}},"source":["## One more thing to consider is the correlation among the variables: \n","pca_instance = PCA().fit(first_stage_data)\n","\n","## Taking care to preserve the index for later join: \n","first_stage_data_pca = pd.DataFrame(pca_instance.transform(first_stage_data), index = first_stage_data.index)\n","\n","fig, ax = plt.subplots(figsize=(20,8),nrows = 1 , ncols = 2)\n","\n","sns.scatterplot(x = first_stage_data_pca.iloc[:,0], y = first_stage_data_pca.iloc[:,1], ax = ax[0])\n","ax[0].set_title('Principal Components')\n","ax[0].set_xlabel(\"PC 1\")\n","ax[0].set_ylabel(\"PC 2\")\n","\n","ax[1].bar(x = [i+1 for i in range(pca_instance.n_components_)], height  = pca_instance.explained_variance_/ np.sum(pca_instance.explained_variance_))\n","\n","ax2 = ax[1].twinx()\n","ax2.plot([i+1 for i in range(pca_instance.n_components_)],  np.cumsum(pca_instance.explained_variance_)/ np.sum(pca_instance.explained_variance_), c = 'orange')\n","ax2.grid([])\n","ax2.set_ylim(bottom =0)\n","\n","ax[1].set_title('Explained variance plot:')\n","ax[1].set_xlabel('Number of principal components')\n","ax[1].set_ylabel('Explained variance')\n","ax2.set_ylabel('Cumulative variance')\n","## "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L80otrtZylcp","colab_type":"text"},"source":["The easiest way to reduce the redundancy of our data is to apply PCA.  \n","\n","PCA is beneficial preprocessing step for t-SNE, recommended both by the author and by the ML practitioners. \n","  \n","Number of components to include is a tunable parameter by itself and initially we can adopt the “elbow” approach. Generally we should identify where the marginal explained variance starts to diminish.  \n","\n"]},{"cell_type":"markdown","metadata":{"id":"oXbrttopEJuI","colab_type":"text"},"source":["## t-Distributed Stochastic Neighbor Embedding (t-SNE)\n","\n","Intended as a technique for dimensionality reduction well suited for the visualization of high-dimensional datasets\n","\n","**Materials**  \n","[Official Website With papers and implementations under different languages](https://lvdmaaten.github.io/tsne/)   \n","[How to Use t-SNE Effectively](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668) an amazing interactive blogpost  \n","[How to tune hyperparameters of tSNE](https://towardsdatascience.com/how-to-tune-hyperparameters-of-tsne-7c0596a18868)  \n","[sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)\n"]},{"cell_type":"code","metadata":{"id":"mK_bUH_5DbNk","colab_type":"code","colab":{}},"source":["from sklearn.manifold import TSNE"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"RMuHvxXG0TcR","colab_type":"code","colab":{}},"source":["## Simple t-SNE usage: \n","## Tsne follows the sklearn interface :\n","\n","# 1. We create instance of the object\n","# Here we set the parameters that are internal to the algorithm\n","\n","TSNE_Instance = TSNE(n_components =  2          ## Controls the number of dimebsuibs if the embedded space\n","                    ,perplexity = 10            ## Perplexity controls how to balance attention between local and global aspects of your data \n","                    ,early_exaggeration = 12    ## Controls how tight natural clusters are situated\n","                    ,learning_rate = 100        ## Controls the step during the gradient descent stage\n","                    ,n_iter = int(1e3)          ## Max number of iterations\n","                    ,random_state =  500        ## Random seed \n","                    ,verbose = 2                ## controls logging of the algorithm\n","                     )\n","\n","# 2.Fit the object to the data \n","TSNE_results = TSNE_Instance.fit_transform(first_stage_data)\n","\n","# The result of fit_transform is ndarray with shape (NSamples, n_components)\n","\n","plt.figure(figsize=(7,7))\n","plt.scatter(TSNE_results[:, 0], TSNE_results[:, 1])\n","plt.xlabel('X')\n","plt.ylabel('Y')\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sXYKrUUCQ4AI","colab_type":"text"},"source":["### t-SNE tunable parameters\n","**n_components** - similar to N components In PCA. It sets the cardinality of your output feature set. sklearn t-sne implementation supports up to 3d plane. \n","\n","_Practical note:_ algorithm becomes exponentially slow with the increase of this parameter  \n","  \n","**perplexity** - Perplexity controls how to balance attention between local and global aspects of your data. Low values yield many small scattered clusters while large values tend to clump up the data. This parameter is very data dependent and it should be explored every time \n","\n","**PCA_components** - Although not internal for the algorithm in practice t-SNE benefits and in a way complements PCA preprocessing. With the former the linear structure of the data is captured while with the former technique can capture non-linear latent structure.\n","\n"," "]},{"cell_type":"markdown","metadata":{"id":"4RQhP0aDUOqX","colab_type":"text"},"source":["### Technical note on the utility functions\n","Since we’ve intended this workshop to be more of a decision making exercise rather that coding one, we are about to introduce several utility functions, defined in order to visualize and automate some of the routines that about to go through.  \n","</br>\n"]},{"cell_type":"markdown","metadata":{"id":"LDI5YcxuNXUd","colab_type":"text"},"source":["#### model_train()\n","\n","**model_train** :  this function will be used to train the embedding with multiple predefined values of the tunable parameters:\n","*  data - pandas data frame or numpy matrix. The data is restricted to numeric arrays without missing observations\n","*  model - string with one of the following values: _\"TSNE\", \"DBSCAN\", \"UMAP\"_\n","* param_dict - python dictionary with tunable parameters. The name of the parameter must be the key of the dictionary, the value must be **python list** with values.  \n","</br>\n","\n","       param_dict = {\n","                     'perplexity':[10,20]\n","                    , n_iter:[300,500] \n","                    }\n","\n","*  apply_pca - boolean parameter that indicates if the original data will be PCA transformed. \n","*  n_pc - Number of principal components applied as **list of integers** (n_pc = [10, 20]). This parameter is considered as tunable and it is added under the tunable parameters in the output of the function. It is ignored if apply_pca = False\n","* other_params - non-tunable parameters. Like __param_dict__ the keys should be the name of the parameter. Unlike __param_dict__ the value should not be list\n","</br>\n","\n","       param_dict = {\n","                     'learning_rate':np.float32(1e-3)\n","                    }\n","\n","The result of the function is python list. Each element contains triple object where:\n","*  First element of the triple is the dictionary with tunable parameter values\n","*  The second element is the coordinates of the embedding\n","*  The last element is the non-tunable parameters \n","\n","This result is fed to several plotting  functions in order to produce neat visualizations that will help us interpret the results of our embedding"]},{"cell_type":"markdown","metadata":{"id":"n1v_d2lBxraI","colab_type":"text"},"source":["####plot_results()  \n","This is one of the first plotting function that is designed for exploration of the tuning results. The way it works it that it takes object, generated  from model_train() function and creates grid of plots arranged by the values of the parameters that have been used for the embeddings.  \n","\n","Important to note is that this function **plots only the first two dimensions** of the embedding.  If the number of dimensions is higher than 2 plot_embedding() can be used to visualize all dimensions. \n","\n","**Arguments:**  \n","*  results - results, generated from model_train()  \n","*  leading_param = None - the name of the parameter that should be used to arrange the tuning results row wise.\n","*  color_array - list of numpy arrays that will be used to apply color to the scatterplot matrix. If given, the length of the list should be either 1 (all scatters are colored with the same variable) or equal to the length of the results object (then each object is colored with its respective color array).  The second option will be used during exploring the results from DBSCAN clustering. \n","* force_categorical = False - if the color array should be represented on a discrete color scale.  This option is meaningful for integer color arrays with limited number of distinct values \n","* point_size = 3 -  controls the size of the points in the scatterplot"]},{"cell_type":"markdown","metadata":{"id":"AE8fFDcmVS12","colab_type":"text"},"source":["#### plot_embedding()\n","\n","This is a function that is designed to plot single embedding. It takes **single** object, generated from model_train() and plots all dimensions are scatterplot matrix.  \n","\n","**Arguments**  \n","* result - single element subset from model_train() results.\n","* fig_scale - plot scaling parameter\n","* color_var_list- list of numpy arrays that is used for coloring \n","*  force_categorical = False - if the color array should be represented on a discrete color scale.  This option is meaningful for integer color arrays with limited number of distinct values \n","* plot_centers = FALSE - used when we plot DBSCAN clusters. It overlays the cluster centers\n","* dont_plot_minus_one - used when we plot DBSCAN clusters. Don't plot outlier centers \n","* point_size = 4 -  controls the size of the points in the scatterplot"]},{"cell_type":"markdown","metadata":{"id":"WpAHMHvfVcuH","colab_type":"text"},"source":["### Tune T-SNE"]},{"cell_type":"code","metadata":{"id":"flOjxkRXCZgv","colab_type":"code","colab":{}},"source":["## EXPERIMENT WITH DIFFERENT TSNE SETTINGS ## \n","\n","\n","## TSNE PARAMETERS\n","## https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html\n","\n","MODEL = 'TSNE'\n","\n","TUNABLE_PARAMETERS = {'perplexity':[25]}\n","DEFAULT_PARAMETERS = {'learning_rate':np.int(1e2)}\n","DATA = first_stage_data\n","\n","APPLY_PCA = True\n","N_PC = [30,50, 55]\n","\n","\n","## Apply tuning:\n","TSNE_results = model_train(   data  = DATA\n","                            , model = MODEL\n","                            , param_dict = TUNABLE_PARAMETERS\n","                            , apply_pca = APPLY_PCA\n","                            , n_pc = N_PC\n","                            , other_params = DEFAULT_PARAMETERS\n","                            )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"w7fXq5z9fKX6","colab_type":"code","colab":{}},"source":["## Inspect the results of the parameters\n","\n","################\n","## PARAMETERS ##\n","################\n","\n","RESULTS = TSNE_results\n","LEADING_PARAM = 'n_pc'\n","COLOR_ARRAY = None\n","FORCE_CATEGORICAL = False\n","POINT_SIZE = 5\n","\n","## APPLY FUNCTION \n","plot_results(results = RESULTS\n","            ,leading_param = LEADING_PARAM\n","            ,color_array = COLOR_ARRAY\n","            ,force_categorical = FORCE_CATEGORICAL\n","            ,point_size = POINT_SIZE\n","             )\n","## If single combination is being expected\n","##plot_embedding(RESULTS[0], fig_scale= 1.2,point_size = 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"imo_eYzRCtd7","colab_type":"text"},"source":["### Capture \"clusters\" through DBSCAN :\n","\n","We will use Density-based spatial clustering of applications with noise (DBSCAN) algorithm to address the different \"clusters\" in our embedding. This is not a true clustering because t-SNE eliminated the actual structure of the data. However, this pseudo clustering step is useful and can guide us through the profiling of our final clusters\n","\n","The tunable parameters that we are going to consider are: \n","\n","**eps:** specifies how close points should be to each other to be considered a part of a cluster. It means that if the distance between two points is lower or equal to this value (**eps**), these points are considered neighbors  \n","\n","**min_samples:** the minimum number of points to form a dense region. For example, if we set the **min_samples** parameter as 5, then we need at least 5 points to form a cluster.  \n","</br>\n","**Several references:**  \n","[DBSCAN Original paper](http://www2.cs.uh.edu/~ceick/7363/Papers/dbscan.pdf)  \n","[Medium post](https://towardsdatascience.com/machine-learning-clustering-dbscan-determine-the-optimal-value-for-epsilon-eps-python-example-3100091cfbc)  \n","[sklearn implementation](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html)  \n","\n"]},{"cell_type":"code","metadata":{"id":"J8Th-p2aNaq5","colab_type":"code","colab":{}},"source":["from sklearn.cluster import DBSCAN"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W2lDMjH7_B9z","colab_type":"text"},"source":[""]},{"cell_type":"code","metadata":{"id":"Ho0YP8tl_KY3","colab_type":"code","colab":{}},"source":["## Choose one embedding to operate with \n","## Zoom plot in : \n","RESULTS = TSNE_results\n","MODEL_INDEX = 1\n","\n","\n","###\n","plot_embedding(RESULTS[MODEL_INDEX], fig_scale = 2, point_size = 9)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZSkXXWcThq3P","colab_type":"text"},"source":["#### Tune DBSCAN"]},{"cell_type":"code","metadata":{"id":"NqEGm0S8ie_N","colab_type":"code","colab":{}},"source":["## Capture the clusters with dbscan \n","\n","TUNABLE_PARAMETERS = {'eps': [4,5,6]\n","                    , 'min_samples':[15,30,45]}\n","DEFAULT_PARAMETERS = {}\n","MODEL = 'DBSCAN'\n","\n","## Apply tuning:\n","dbscan_clusters = model_train(  data = RESULTS[MODEL_INDEX][1]\n","                                , model = MODEL\n","                                , param_dict = TUNABLE_PARAMETERS\n","                                , other_params = DEFAULT_PARAMETERS\n","                                )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"e-DeylBdqmfI","colab_type":"code","colab":{}},"source":["## Plot the results from DBSCAN clustering \n","result_for_plotting = [(i[0],RESULTS[MODEL_INDEX][1],i[2]) for i in dbscan_clusters]\n","dbscan_clusters_results = [i[1] for i in dbscan_clusters]\n","plot_results(results = result_for_plotting, color_array=dbscan_clusters_results, force_categorical=True, point_size=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CKcwHx_Vbhv_","colab_type":"code","colab":{}},"source":["## Zoom one of the plots: \n","RESULTS = TSNE_results\n","CLUSTER_INDEX = 3\n","plot_embedding(RESULTS[MODEL_INDEX]\n","               , fig_scale = 2.2\n","               , color_var_list = [dbscan_clusters[CLUSTER_INDEX][1]]\n","               , force_categorical=True\n","               , plot_centers=True\n","               ,  point_size = 10)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Nw1Y6JvMh7j_","colab_type":"text"},"source":["#### Profile Clusters"]},{"cell_type":"code","metadata":{"id":"vETlXMEvqTHC","colab_type":"code","colab":{}},"source":["## COMPARE THE CLUSTERS \n","## PICK METRICS BY WHICH CLUSTERS WOULD BE COMPARED \n","\n","DATA = descriptive_statistics\n","METRICS = ['cpu_run_mean', 'cpu_ready_mean']\n","\n","\n","\n","### CREATE SUMMARY TABLE \n","pd.set_option('display.max_rows', None)\n","pd.options.display.float_format = '{:,.3f}'.format\n","data = DATA[METRICS].copy()\n","data['dbscan_clusters'] = dbscan_clusters[CLUSTER_INDEX][1]\n","\n","\n","display(np.round(pd.melt(data, id_vars = ['dbscan_clusters']).groupby(['variable', 'dbscan_clusters']).describe(), 3))\n","pd.set_option('display.max_rows', 30)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5NlLgcHinDH","colab_type":"code","colab":{}},"source":["## PARAMETERS ## \n","TRANSFORMATION = return_transformer(None)\n","#DATA = virt_hw.loc[:, virt_hw.columns != 'os_fam'] ## it is categorical variable\n","DATA = correlation_data\n","\n","\n","\n","\n","\n","## Plot the distribution as boxplot\n","clustering_df = pd.DataFrame(index = first_stage_data.index)\n","clustering_df['dbscan_clusters'] = dbscan_clusters[CLUSTER_INDEX][1]\n","\n","data = DATA.copy()\n","data = TRANSFORMATION(data)\n","data = pd.merge(data,clustering_df, left_index=True, right_index=True)\n","data = pd.melt(data, id_vars = ['dbscan_clusters'])\n","ordr = list(sorted(set(dbscan_clusters[CLUSTER_INDEX][1])))\n","\n","g = sns.FacetGrid(data, col = 'variable', col_wrap = 5, sharex = False, sharey=False, height= 5)\n","g.map(sns.boxplot, 'dbscan_clusters', 'value').add_legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ACp_R0Iwsw4Y","colab_type":"code","colab":{}},"source":["## Parameters: \n","DATA = correlation_data \n","COLOR_VAR = 'corr_net_packetsRx_net_packetsTx'\n","TRANSFORMATION = return_transformer(None)\n","RESULTS = TSNE_results\n","\n","### \n","plot_embedding(  RESULTS[MODEL_INDEX]\n","               , fig_scale = 2\n","               , color_var_list = [TRANSFORMATION(DATA[COLOR_VAR].values)]\n","               , point_size = 20\n","               )\n","\n","## Utility code that helps us compare the embedding generated from pca to the embedding\n","#pca_result_construct = ({}, first_stage_data_pca.iloc[:,0:5].values,{})\n","#plot_embedding(pca_result_construct, fig_scale=.8, color_var_list =  [TRANSFORMATION(DATA[COLOR_VAR].values)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nvOQiCkdonTF","colab_type":"code","colab":{}},"source":["## CHECK ORIGINAL SIGNALS ARE DIFFERENT FOR THE DIFFERENT CLUSTERS ## \n","\n","clusters_to_compare = [1,9]\n","samples_per_cluster = 5\n","metric = 'net_packetsRx'\n","DATA = perf_data\n","sharey = False\n","\n","\n","### Fig scaling: \n","num_cl_to_compare = len(clusters_to_compare)\n","\n","\n","\n","####\n","sampled_df = clustering_df[clustering_df['dbscan_clusters'].isin(clusters_to_compare)].groupby('dbscan_clusters').apply(lambda x: x.sample(samples_per_cluster))\n","fig, axes = plt.subplots(nrows = samples_per_cluster, ncols = num_cl_to_compare, figsize = (15*num_cl_to_compare, 5*samples_per_cluster), sharey=sharey)\n","\n","ix_i = 0\n","\n","for i in clusters_to_compare:\n","    _tmp = list(sampled_df.iloc[sampled_df.index.get_level_values(0).isin([i]),:].index.get_level_values(1))\n","    ix_j = 0\n","    for j in _tmp:\n","        axes[ix_j, ix_i].plot(DATA[j][metric])\n","        axes[ix_j, ix_i].set_ylabel('vmid:'+str(j) + '('+str(i)+')')\n","        axes[ix_j, ix_i].yaxis.set_label_position('right')\n","        ix_j += 1\n","    ix_i += 1"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"W18c261EVd2J","colab_type":"text"},"source":["## Advanced Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"eAL2ww9-7xsw","colab_type":"text"},"source":["### Missing data Imputation  \n","\n","The statistical tests are sensitive or do not tolerate at all missing data. \n","\n","#### Univariate imputation methods: \n","*  Instance Imputation\n","*  Summary statistics\n","*  Interpolation\n","*  Moving average\n","\n"]},{"cell_type":"code","metadata":{"id":"WM6syiBnNbrq","colab_type":"code","colab":{}},"source":["## Get an intuition about the missing data\n","## Calculate the % of missing observations: \n","dta = []\n","for k, data in perf_data.items():\n","    dta.append(data.isnull().sum().sum() / (T*D))\n","\n","## Inspect the distribution of missing values \n","sns.distplot(dta, kde = False, bins = 30)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scEi7Y_b8dw8","colab_type":"text"},"source":["We miss up to 5% of data as the majority of the cases lack only 1-2% "]},{"cell_type":"code","metadata":{"id":"KQ__pdUk4wT7","colab_type":"code","colab":{}},"source":["## This is a toy array where we can observe  the effect of different imputation methods on the data\n","\n","## These parameters control the steepness of the data\n","START = 1\n","STOP = 10\n","POW = 1\n","POLY_ORDER = 2  \n","TRANSFORMATION = return_transformer('None')\n","\n","### \n","tmp = TRANSFORMATION(pd.Series(np.random.normal(size = 100) + np.linspace(start = START, stop = STOP, num = 100)**POW))\n","tmp_nan = tmp[43:56].copy()\n","tmp[45:55] = np.nan\n","\n","fig = plt.figure(figsize = (18, 6))\n","ax = fig.add_axes([0,0,1,1])\n","ax.set_title('Time series imputation methods:')\n","ax.plot(tmp, label = 'base ts')\n","\n","base_line = plt.plot(tmp_nan, linestyle = \"--\", label = 'true values')\n","ax.plot(tmp.interpolate(method = 'linear')[44:56], label = 'linear interpolation')\n","\n","## Polynomial\n","poly_order_label = 'poly '+ str(POLY_ORDER) +' interpolation '\n","plt.plot(tmp.interpolate(method = 'polynomial', order = POLY_ORDER)[44:56], label = poly_order_label)\n","\n","\n","tmp_ma_data = tmp.copy()\n","tmp_ma_data[44:56] = tmp.rolling(12, min_periods = 1).mean()[44:56]\n","ax.plot(tmp_ma_data[43:57], label = 'Simple MA')\n","\n","#ax.plot(tmp.ewm(com = 0.5).mean(), label = 'EW MA') MA with Exponential decay \n","#ax.plot(tmp.rolling(12, min_periods = 1).mean()[44:56], label = 'Simple MA')\n","\n","plt.legend()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3MsWiV3lEow","colab_type":"code","colab":{}},"source":["## CREATE NEW DATASET WITH IMPUTED VALUES \n","\n","IMPUTATION_METHOD = 'interpolation' # mean, median, mode, interpolation, ma\n","POLY_ORDER = 1 ## The order of the interpolation. 1 = linear \n","MA_RANGE = 6\n","TRANSFORMATION = return_transformer(None)\n","\n","\n","##\n","OVERWRITE_MA_RANGE = True # IF the max_gap within series > MA_RANGE use MA_RANGE = max_gap.\n","## Calculate the largest gap \n","gap_df_list = []\n","perf_data_no_missing = {}\n","i = 0\n","for k, data in perf_data.items():\n","\n","    data_running_copy = data.copy()\n","    data_running_copy = TRANSFORMATION(data_running_copy)\n","    running_dict = {}\n","\n","    for col in F:\n","        max_gap = data[col].isnull().astype(int).groupby(data[col].notnull().astype(int).cumsum()).sum().max()\n","        running_dict[col] = max_gap\n","\n","        ## Apply Imputation:\n","        if IMPUTATION_METHOD == 'mean':\n","            data_running_copy[col].fillna(data_running_copy[col].mean(), inplace=True)\n","        elif IMPUTATION_METHOD == 'median':\n","            data_running_copy[col].fillna(data_running_copy[col].median(), inplace=True)\n","        elif IMPUTATION_METHOD == 'mode':\n","            data_running_copy[col].fillna(data_running_copy[col].mode(), inplace=True)\n","        elif IMPUTATION_METHOD == 'interpolation':\n","            data_running_copy[col] = data_running_copy[col].interpolate(method = 'polynomial', order = POLY_ORDER)\n","        elif IMPUTATION_METHOD == 'ma':\n","            ## Init the ma parameter\n","            ma_range_running = MA_RANGE\n","\n","            ## Subtle hack: If the largest gap > MA_RANGE make MA_RANGE = largest gap for this metric \n","            if MA_RANGE <= max_gap  and OVERWRITE_MA_RANGE:\n","                ma_range_running = max_gap +1 \n","            data_running_copy[col].fillna(data_running_copy[col].rolling(ma_range_running, min_periods = 1).mean(), inplace=True)\n","\n","\n","    gap_df_list.append(pd.DataFrame(running_dict, index = [k]))\n","    perf_data_no_missing[k] = data_running_copy \n","\n","    i += 1\n","    if i%100 == 0:\n","        print('Iteration', i, 'out of', N )\n","\n","print('Iteration', i, 'out of', N,'\\nDone')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lzEgqtPzyrYD","colab_type":"code","colab":{}},"source":["## Zoom to random place in our data in order to see how our gaps have been patched: \n","\n","MISS_RANGE = 1\n","DATA_RANGE = np.int(144/4)\n","\n","## Get random metric \n","any_missings = 0\n","while any_missings == 0:\n","    metric_tmp = F[np.random.randint(low = 0, high = D)]\n","    vmid , data = sample_observation(data = perf_data)\n","    any_missings = data[metric_tmp].isnull().sum()\n","\n","## Get the first and the last data \n","first_ix = np.min(data[metric_tmp].index)\n","last_ix = np.max(data[metric_tmp].index)\n","\n","##  Get the indices where the data is missing\n","missing_data_ix = data[metric_tmp].index[data[metric_tmp].apply(np.isnan)]\n","## Trick to ensure that we have gap centered between non-missing sequence\n","miss_date_ix = np.min([dix for dix in missing_data_ix if dix - pd.Timedelta(minutes=5 * DATA_RANGE / 2) >  first_ix and dix + pd.Timedelta(minutes=5 *  DATA_RANGE / 2) < last_ix])\n","\n","## \n","data_ix_reconstruct = [miss_date_ix + pd.Timedelta(minutes = 5*i) for  i in range(-DATA_RANGE, DATA_RANGE+1)]\n","\n","## \n","tmpdata_orig = data[data.index.isin(data_ix_reconstruct)][metric_tmp]\n","tmpdf = pd.DataFrame(index = tmpdata_orig.index)\n","\n","\n","##\n","miss_lst = []\n","for i in tmpdata_orig[np.isnan(tmpdata_orig)].index:\n","   # print(i)\n","    miss_lst.append(i)\n","    miss_lst.append(i + pd.Timedelta(minutes=5))\n","    miss_lst.append(i - pd.Timedelta(minutes=5))\n","miss_lst = list(sorted(set(miss_lst)))\n","\n","## \n","fig  = plt.figure(figsize = (16,4))\n","ax = fig.add_axes([0,0,1,1])\n","ax.plot(data_ix_reconstruct, TRANSFORMATION(data[data.index.isin(data_ix_reconstruct)][metric_tmp]), label = 'original')\n","ax.plot(miss_lst, perf_data_no_missing[vmid][perf_data_no_missing[vmid].index.isin(miss_lst)][metric_tmp], label = 'imputed', linestyle = \"--\")\n","ax.set_title('vmid: '+ str(vmid) + ' ' + metric_tmp)\n","plt.legend()\n","plt.show()\n","#pd.concat(gap_df_list).describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"voJWWexO8wCA","colab_type":"code","outputId":"e7102127-f067-4192-df6c-68e3d7922cd4","executionInfo":{"status":"ok","timestamp":1579265444253,"user_tz":-120,"elapsed":789,"user":{"displayName":"AMLD TemUserMaster","photoUrl":"","userId":"00430682455941763130"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["## Double check if any variable is still missing\n","\n","dta_imputed = []\n","for k, data in perf_data_no_missing.items():\n","    dta_imputed.append(data.isnull().sum().sum() / (T*D))\n","\n","## Check if there are instances with missing values:\n","still_missing = [i for i, k in enumerate(dta_imputed) if k > 1e-3]\n","data = [list(perf_data_no_missing.keys())[i] for i in still_missing]\n","print('VMs with at least 1 missing observation:' ,len(data))\n","\n","#\n","#for i in tmp:\n","#    del perf_data_no_missing[i]\n","#print(len(perf_data_no_missing.keys()))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Virtual machines with at least 1 missing observation: 0\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"lDKZOBpX4-_E","colab_type":"text"},"source":["\n","### Time series specific features\n","\n","**_PROS_**   \n","More expressive  \n","retain the information encoded in the time domain of the data  \n","  \n","**_CONS_**   \n","costly to compute\n","hard to interpret - thus to profile cluster  \n","requires domain knowledge\n","not trivial to ensure comparability\n","some of the tests are very sensitive to data inconsistencies"]},{"cell_type":"markdown","metadata":{"id":"RuFvYqwqQyQQ","colab_type":"text"},"source":["### Spectral Density  \n","\n","Another set of features that we will consider are related to the presence (or absence) of periodic patterns in the data. One way to check if series show any systematic fluctuations is through spectral decomposition of the data.  \n","The general idea is that any real time series (signal) can be approximated through linear combination of sine waves with different frequencies. The amplitude of these sine waves account for the variation, captured through the decomposition.  \n","\n","</br>  \n","**Resources:**  \n","[Relationship between fft and psd](https://dsp.stackexchange.com/questions/24780/power-spectral-density-vs-fft-bin-magnitude)  \n","[Power vs. Variance](https://dsp.stackexchange.com/questions/46385/variance-in-the-time-domain-versus-variance-in-frequency-domain)  \n","[But what is the Fourier Transform? A visual introduction.](https://www.youtube.com/watch?v=spUNpyF58BY)  \n","[Introduction to the Fourier Transform](https://www.youtube.com/watch?v=1JnayXHhjlg)  \n","[Spectral Analysis in R](https://ms.mcmaster.ca/~bolker/eeid/2010/Ecology/Spectral.pdf)"]},{"cell_type":"markdown","metadata":{"id":"RaypydQ9VQz_","colab_type":"text"},"source":["#### Compute spectral density related features"]},{"cell_type":"code","metadata":{"id":"y4VgdCkcNckR","colab_type":"code","colab":{}},"source":["\n","## Here we define the workhorse  function that will \"slide\" through the \n","## spectrogram and extract spectral density volumes within specific intervals \n","\n","## Arguments:\n","## df:          pandas data frame that contains the time series data \n","## series_name:   Imputed Series \n","## freq_arr: Time frequencies at which the spectrogram will be captured \n","## sm_win:   Smoothing window  +/- Nsamples\n","## sm_fun:   The function that will be used to aggregate the values from the window  \n","## remove_mean: Bool indicating if mean value should be subtracted from the series\n","## poly_filt: filter polynomial trend \n","## poly_deg: the degree of the trend (deg 1 = linear trend)\n","## return_series: If we want to return the frequency/ fft arrays (potentially for plotting)\n","\n","\n","def calc_PSD_features(  df\n","                      , series_name \n","                      , freq_arr = None\n","                      , sm_win = 2\n","                      , sm_fun = np.mean\n","                      , remove_mean = True\n","                      , poly_filt = True\n","                      , poly_deg = 1\n","                      , return_series = False\n","                      ):\n","    \n","    data = df[series_name]\n","    T_data = len(data)\n","\n","    ## Subtract mean:\n","    if remove_mean:\n","        data = data - np.mean(data)\n","    \n","    ## Remove linear trend: \n","    if poly_filt:\n","        poly_coef = np.polyfit(range(T_data), data, deg = poly_deg)\n","    \n","        # This is the poly object that will actually fit the data with the poly_coef \n","        poly_fit = np.poly1d(poly_coef)\n","\n","        ## Apply poly filtering \n","        data = data - poly_fit(range(T_data))\n","\n","    ## Apply fft: \n","    data_fft = np.fft.fft(data)\n","\n","    ## Transform fft to psd \n","    data_fft = np.power(np.abs(data_fft), 2) / T_data\n","\n","    ## Include option for returning the PSD arrays for plotting:\n","    if return_series:\n","        return(data_fft)\n","\n","    ## Construct the time window at which the data will be aggregated: \n","    ## Keep the index in a tuple \n","    \n","    filt_win = range(-sm_win , sm_win+1)\n","    per_freq = np.array(range(T_data)) \n","\n","    ## This creates tuple with freq_value (like 1, 2, 3  etc hours and their position in the array)\n","    time_window_array = [(ix, np.where(per_freq == ix+1) + np.array(filt_win)) for ix in freq_arr]\n","    \n","    ## Calculate Total Power of the singal / ts\n","    total_power = np.sum(data_fft)\n","    average_power = np.mean(data_fft) ## Under mean = 0 average power = variance \n","\n","\n","    ## Calculate the captured power within the given time range \n","    ## The column_name corresponds to: {variable_name}_{freq}\n","\n","    data_dict = {}\n","    #ddict_total = '{}_uncaptured'.format(series_name)\n","    #data_dict[ddict_total]  = 1\n","\n","    for fr, data_ix in time_window_array:\n","        ddict_key = '{}_pwidth_{}'.format(series_name, fr)\n","\n","        data_dict[ddict_key] = sm_fun(data_fft[data_ix[data_ix > 0]]/average_power)\n","        #data_dict[ddict_total] -= data_dict[ddict_key] \n","        #data_dict[ddict_key] = (sm_fun(data_fft[data_ix[data_ix > 0]])*2)/total_power\n","\n","    \n","    ## Add Uncaptured variance:\n","    return(data_dict)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-P_QjsjJXYdH","colab_type":"text"},"source":["#### Inspect spectrogram for single vm"]},{"cell_type":"code","metadata":{"id":"qVHi1iLBTEKj","colab_type":"code","colab":{}},"source":["## Inspect the spectrogram for single VM\n","\n","DATA = perf_data_no_missing\n","vmid = 234\n","metric = 'net_packetsRx'\n","\n","\n","##\n","data = calc_PSD_features(DATA[vmid]\n","                        ,series_name = metric\n","                        ,return_series = True\n","                         )\n","## \n","\n","fig, axes = plt.subplots(nrows = 2, figsize = (15,7))\n","axes[0].plot(DATA[vmid][metric] - np.mean(DATA[vmid][metric].values))\n","axes[0].set_ylabel('Original Series')\n","axes[0].yaxis.set_label_position('right')\n","axes[1].plot(data[:1008])\n","axes[1].scatter([7,14,21,28,168,336,504, 672], [0,0,0,0,0,0,0,0], c = 'red', s = 5)\n","axes[1].set_ylabel('Unsmoothed spectrogram')\n","axes[1].yaxis.set_label_position('right')\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"xTQXnKOLpKeT","colab_type":"code","colab":{}},"source":["## Calculate the psd features for the entire dataset \n","## Iterate over vms and variables \n","## and apply function: \n","\n","########################\n","## TUNABLE PARAMETERS ##\n","########################\n","\n","TRANSFORMATION = 'ln' # one of: None, 'ln', log10, 1/x, -1/x ,'cuberoot'\n","#FREQ_ARRAY = range(1, 7) ## We will extract the density for each hour \n","FREQ_ARRAY = [7,14,21,28,168,336,672] ## We will extract the density for each hour \n","SMOOTHING_WINDOW = 2 ## We will smooth them with +/- SWIN samples\n","\n","REMOVE_MEAN = True\n","POLY_FIT = True # We will fit polynomial ...\n","POLY_DEG = 1 # ... of degree POLY_DEG and subtract it from the original data ultimately eliminating the respective trend \n","\n","### FREQ_ARRAY INTERPRETATION \n","# FREQ_ARRAY shows where we expect to observe cycles \n","# the values in FREQ_ARRAY represents number of cycles per our entire observation period \n","# thus FREQ_ARRAY = 7 means that we expect to have 7 cycles/ for 1 week timeseries data that we have this corresponds to 1 cycle per day  or daily cyclicity \n","# 14 = half day cyclicity / 12 hours \n","\n","\n","\n","###########################\n","## CREATE PSD DATA FRAME ##\n","###########################\n","i = 1\n","psd_features = []\n","for k, data in perf_data_no_missing.items():\n","    psd_res_ph = {}\n","    transformation = return_transformer(TRANSFORMATION)\n","    data = transformation(data)\n","    for col in data.columns: \n","        psd_running = calc_PSD_features(data, col\n","                                      , freq_arr = FREQ_ARRAY\n","                                      , sm_win = SMOOTHING_WINDOW\n","                                      , remove_mean = REMOVE_MEAN\n","                                      , poly_filt = POLY_FIT\n","                                      , poly_deg = POLY_DEG\n","                                      , sm_fun = np.mean\n","                                        )\n","        dict.update(psd_res_ph, psd_running)\n","    \n","    psd_features.append(pd.DataFrame(psd_res_ph, index = [k]))\n","\n","    if i%100 ==0:\n","        print('Iteration ', i, 'out of', N)\n","    \n","    i += 1\n","print('Done')\n","## Combine all elements into single pandas data frame :\n","psd_features = pd.concat(psd_features)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"5QD4uQr9f1lx","colab_type":"code","colab":{}},"source":["## Combine the PSD features with the ADF features  into single data frame \n","psd_features.describe()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"6CXxfG52aakV","colab_type":"code","colab":{}},"source":["TRANSFORMATION = return_transformer(None)\n","\n","## inspect the distribution of the features:\n","## NB: The data has been log transformed \n","\n","g = sns.FacetGrid(pd.melt(TRANSFORMATION(psd_features)), col = 'variable', col_wrap = 6, sharex = False, sharey=False, aspect = 1.5)\n","g.map(plt.hist, 'value', bins = 40)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l8Fpa9rP5gJm","colab_type":"text"},"source":["## Second wave of modeling:"]},{"cell_type":"markdown","metadata":{"id":"Ns2Z9VcIW2SN","colab_type":"text"},"source":["### Scale PSD features"]},{"cell_type":"code","metadata":{"id":"e4LV-vqyjsbv","colab_type":"code","colab":{}},"source":["#########################################\n","##:: psd_features DATA PREPROCESSING ::##\n","#########################################\n","\n","## Apply transformation\n","TRANSFORMATION = return_transformer(None)\n","\n","APPLY_STANDARD_SCALING = False\n","\n","APPLY_MIN_MAX_SCALING = True\n","MIN_MAX_SCALER_RANGE = (-1, 1)\n","\n","\n","## We would like to retain the features\n","## Thus, we create hard copy of the data and operate on it:\n","psd_features_transformed = psd_features.copy()\n","psd_features_columns = psd_features.columns\n","\n","psd_features_transformed[psd_features_columns] = TRANSFORMATION(psd_features_transformed[psd_features_columns])\n","\n","## APPLY MIN-MAX SCALING TO DESCRIPTIVE STATISTICS DATA\n","if APPLY_STANDARD_SCALING:\n","    standart_scaler_instance = StandardScaler()\n","    psd_features_transformed[psd_features_columns]  = standart_scaler_instance.fit_transform(psd_features_transformed[psd_features_columns])\n","\n","if APPLY_MIN_MAX_SCALING:\n","    psd_features_transformed[psd_features_columns] = min_max_scaler_instance.fit_transform(psd_features_transformed[psd_features_columns])\n"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4Yl0qMDBkpPo","colab_type":"text"},"source":["### Construct dataset for modeling"]},{"cell_type":"code","metadata":{"id":"jhPZN7jVtR_9","colab_type":"code","colab":{}},"source":["## CREATE FINAL DATASET ## \n","## CHOOSE WHICH FEATURES TO INCLUDE FOR THE FINAL DATASET\n","## BY DEFAULT, WE WILL MODEL ALL FEATURES THAT WE’VE CREATED \n","## HOWEVER, WE CAN MAKE MODELS OUTPUT EASIER TO INTERPRET IF WE MODEL INFORMATION ONLY FOR ONE RESOURCE TYPE (CPU or NETWORK) \n","\n","\n","## To include columns that contains cpu_run\n","## descriptive_statistics_transformed.filter(regex = 'cpu_run')\n","\n","## To exclude columns containing cpu_run string \n","## descriptive_statistics_transformed.loc[:,~descriptive_statistics_transformed.columns.str.contains('cpu_run')]\n","\n","\n","\n","\n","final_data_list = [\n","                              descriptive_statistics_transformed\n","                             ,correlation_data_transformed\n","                             ,virt_hw_transformed\n","                             ,psd_features_transformed\n","                    ]\n","\n","\n","## To include columns that contains cpu_urn\n","## psd_features_transformed.filter(regex = 'cpu_run')\n","\n","## To exclude columns containing cpu_run string \n","## psd_features_transformed.loc[:,~psd_features_transformed.columns.str.contains('cpu_run')]\n","\n","## Init empty data frame: \n","final_modeling_data = pd.DataFrame(index = list(perf_data.keys()))\n","\n","for i in final_data_list:\n","    data = i.copy()\n","    final_modeling_data = pd.merge(final_modeling_data, data, left_index=True, right_index=True)\n","\n","##\n","print(final_modeling_data.shape)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ww3toZ5AYNz6","colab_type":"text"},"source":["### Inspect linear correlation and PC"]},{"cell_type":"code","metadata":{"id":"Ud0SJ3Ddw0DF","colab_type":"code","colab":{}},"source":["### Inspect correlation:\n","DATA = final_modeling_data\n","cmap = sns.diverging_palette(220, 20, sep=20, as_cmap=True)\n","fig = plt.figure()\n","sns.clustermap(DATA.corr(), figsize = (20, 20), cmap = cmap)\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppypeOhFw5ds","colab_type":"code","colab":{}},"source":["### Inspect PCA: \n","DATA = final_modeling_data\n","\n","## One more thing to consider is the correlation among the variables: \n","pca_instance = PCA().fit(DATA)\n","## Taking care to preserve the index for later join: \n","pca_data = pd.DataFrame(pca_instance.transform(DATA), index = DATA.index)\n","\n","fig, ax = plt.subplots(figsize=(20,8),nrows = 1 , ncols = 2)\n","\n","sns.scatterplot(x = DATA.iloc[:,0], y = pca_data.iloc[:,1], ax = ax[0])\n","ax[0].set_title('Principal Components')\n","ax[0].set_xlabel(\"PC 1\")\n","ax[0].set_ylabel(\"PC 2\")\n","\n","\n","#tmp_plt = sns.lineplot( x= [i+1 for i in range(pca_instance.n_components_)]\n","#                      , y =pca_instance.explained_variance_\n","#                      , ax = ax[1]\n","#                      #, color = 'blue'\n","#                      )\n","\n","ax[1].bar(x = [i+1 for i in range(pca_instance.n_components_)], height  = pca_instance.explained_variance_/np.sum(pca_instance.explained_variance_))\n","\n","ax2 = ax[1].twinx()\n","ax2.plot([i+1 for i in range(pca_instance.n_components_)],  np.cumsum(pca_instance.explained_variance_)/ np.sum(pca_instance.explained_variance_), c = 'orange')\n","ax2.grid([])\n","ax2.set_ylim(bottom =0)\n","\n","ax[1].set_title('Explained variance plot:')\n","ax[1].set_xlabel('Number of principal components')\n","ax[1].set_ylabel('Marginal variance')\n","ax2.set_ylabel('Cumulative variance')\n","## "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8VtNWDuV65b_","colab_type":"text"},"source":["## Uniform Manifold Approximation and Projection (UMAP)  \n","\n","**Resources**  \n","[UMAP Python Manual](https://umap-learn.readthedocs.io/en/latest/)  \n","[Understanding UMAP](https://pair-code.github.io/understanding-umap/)  \n","[How Exactly UMAP Works](https://towardsdatascience.com/how-exactly-umap-works-13e3040e1668)  "]},{"cell_type":"markdown","metadata":{"id":"SqePCiuAX4x0","colab_type":"text"},"source":["### Basic Umap Usage"]},{"cell_type":"code","metadata":{"id":"Vpl_PaDP680N","colab_type":"code","colab":{}},"source":["import umap"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"TiZSGt4xt9zc","colab_type":"code","colab":{}},"source":["## umap usage: \n","\n","DATA =  final_modeling_data\n","\n","# 1. We create instance of the object\n","# Here we set the parameters that are internal to the algorithm\n","\n","UMAP_Instance = umap.UMAP(n_components =  2         ## The dimension of the space to embed into.\n","                        , min_dist = 0.5            ## The effective minimum distance between embedded points. Smaller values will result in a more clustered/clumped embedding\n","                        , spread = 2.0              ## The effective scale of embedded points. In combination with min_dist this determines how clustered/clumped the embedded points are.\n","                        , n_neighbors = 15          ## The size of local neighborhood (in terms of number of neighboring sample points) used for manifold approximation.\n","                        , metric = 'euclidean'      ## The metric to use to compute distances in high dimensional space.  (euclidean, manhattan, cosine, )\n","                        ,learning_rate = 100        ## Controls the step during the gradient descent stage\n","                        ,n_epochs = int(1e3)        ## selected based on the size of the input dataset (200 for large datasets, 500 for small)\n","                        #,random_state =  500       ## Random seed \n","                        ,verbose = 2                ## controls logging of the algorithm\n","                        ,a = None                   ## a and b are another way to control min dist and spread \n","                        ,b = None                   ## \n","                        )\n","\n","# 2.Fit the object to the data \n","result = UMAP_Instance.fit_transform(DATA)\n","\n","# The result of fit_transform is ndarray with shape (NSamples, n_components)\n","plt.figure(figsize=(8,8))\n","plt.scatter(result[:, 0], result[:, 1])\n","plt.xlabel('X1')\n","plt.ylabel('X2');"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WkEmyT5eX8yv","colab_type":"text"},"source":["### UMAP parameter tuning  \n","\n","</br></br>\n","\n","**Resources**  \n","[Basic UMAP Parameters](https://umap-learn.readthedocs.io/en/latest/parameters.html)  \n","[Fine-tuning UMAP Visualizations](https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html)"]},{"cell_type":"code","metadata":{"id":"gt_K0THAYAmE","colab_type":"code","colab":{}},"source":["## EXPERIMENT WITH DIFFERENT UMAP SETTINGS  ## \n","\n","TUNABLE_PARAMETERS = {'n_neighbors':[10,20,30], 'min_dist':[.1,.2,.3], 'spread':[.1,.2,.3]}\n","DEFAULT_PARAMETERS = {}\n","\n","APPLY_PCA = False\n","N_PC = []\n","\n","MODEL = 'UMAP'\n","DATA = final_modeling_data\n","\n","\n","## Apply tuning:\n","UMAP_results = model_train(   data = DATA\n","                            , model = MODEL\n","                            , param_dict = TUNABLE_PARAMETERS\n","                            , apply_pca = APPLY_PCA\n","                            , n_pc = N_PC\n","                            , other_params = DEFAULT_PARAMETERS\n","                            )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"S4uc0d1LYAjO","colab_type":"code","colab":{}},"source":["## Inspect the results of the parameters\n","\n","################\n","## PARAMETERS ##\n","################\n","\n","RESULTS = UMAP_results\n","LEADING_PARAM = 'n_neighbors'\n","COLOR_ARRAY = None\n","FORCE_CATEGORICAL = False\n","POINT_SIZE = 5\n","\n","\n","## APPLY FUNCTION \n","plot_results(results = RESULTS\n","            ,leading_param = LEADING_PARAM\n","            ,color_array = COLOR_ARRAY\n","            ,force_categorical = FORCE_CATEGORICAL\n","            ,point_size = POINT_SIZE\n","             )\n","\n","## If single combination is being expected\n","##plot_embedding(RESULTS[0], fig_scale= 1.2,point_size = 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pJ5vj7f6fd7q","colab_type":"text"},"source":["### Apply DBSCAN"]},{"cell_type":"code","metadata":{"id":"DQCtFitEYAb7","colab_type":"code","colab":{}},"source":["## Choose one embedding to operate with \n","## Zoom plot in : \n","RESULTS = UMAP_results\n","MODEL_INDEX = 2\n","\n","\n","###\n","plot_embedding(RESULTS[MODEL_INDEX], fig_scale = 1.5, point_size = 4)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"lMZM_dmriRT_","colab_type":"code","colab":{}},"source":["## Capture the clusters with dbscan \n","TUNABLE_PARAMETERS = {'eps': [.4, .8,1.2,1.4]\n","                    , 'min_samples':[10, 26,28]}\n","DEFAULT_PARAMETERS = {}\n","MODEL = 'DBSCAN'\n","\n","## Apply tuning:\n","dbscan_clusters = model_train(  data = RESULTS[MODEL_INDEX][1]\n","                                , model = MODEL\n","                                , param_dict = TUNABLE_PARAMETERS\n","                                , other_params = DEFAULT_PARAMETERS\n","                                )"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"sOeCu_8pivW7","colab_type":"code","colab":{}},"source":["## Plot the results from DBSCAN clustering \n","result_for_plotting = [(i[0],RESULTS[MODEL_INDEX][1],i[2]) for i in dbscan_clusters]\n","dbscan_clusters_results = [i[1] for i in dbscan_clusters]\n","plot_results(results = result_for_plotting, color_array=dbscan_clusters_results, force_categorical=True, point_size=5)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"PckKtd0UkMZg","colab_type":"code","colab":{}},"source":["## Zoom one of the plots: \n","RESULTS = UMAP_results\n","CLUSTER_INDEX = 9\n","\n","####\n","plot_embedding(RESULTS[MODEL_INDEX]\n","               , fig_scale = 2\n","               , color_var_list = [dbscan_clusters[CLUSTER_INDEX][1]]\n","               , force_categorical=True\n","               , plot_centers=True\n","               ,  point_size = 5)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hubQstxh2-DL","colab_type":"text"},"source":["### Profile clusters"]},{"cell_type":"code","metadata":{"id":"MHB0QNJOjTJa","colab_type":"code","colab":{}},"source":["## PARAMETERS ## \n","TRANSFORMATION = return_transformer(None)\n","#DATA = virt_hw.loc[:, virt_hw.columns != 'os_fam'] ## it is categorical variable\n","DATA = descriptive_statistics_transformed\n","\n","\n","\n","########\n","## Plot the distribution as boxplot\n","\n","clustering_df = pd.DataFrame(index = final_modeling_data.index)\n","clustering_df['dbscan_clusters'] = dbscan_clusters[CLUSTER_INDEX][1]\n","\n","\n","\n","data = DATA.copy()\n","data = TRANSFORMATION(data)\n","data = pd.merge(data,clustering_df, left_index=True, right_index=True)\n","data = pd.melt(data, id_vars = ['dbscan_clusters'])\n","ordr = list(sorted(set(dbscan_clusters[CLUSTER_INDEX][1])))\n","\n","g = sns.FacetGrid(data, col = 'variable', col_wrap = 5, sharex = False, sharey=False, height= 5)\n","g.map(sns.boxplot, 'dbscan_clusters', 'value').add_legend()\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"dM9ACoLD7y10","colab_type":"code","colab":{}},"source":["## Parameters: \n","DATA =  virt_hw# non_temporal_features/ virt_hw_transformed\n","COLOR_VAR = 'number_of_nics'\n","TRANSFORMATION = return_transformer(None)\n","RESULTS = UMAP_results\n","\n","\n","\n","### \n","plot_embedding(  RESULTS[MODEL_INDEX]\n","               , fig_scale = 2\n","               , color_var_list = [TRANSFORMATION(DATA[COLOR_VAR].values)]\n","               , point_size = 20\n","               , force_categorical = True\n","               )\n","\n","## Utility code that helps us compare the embedding generated from pca to the embedding\n","#pca_result_construct = ({}, first_stage_data_pca.iloc[:,0:5].values,{})\n","#plot_embedding(pca_result_construct, fig_scale=.8, color_var_list =  [TRANSFORMATION(DATA[COLOR_VAR].values)])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"YRfd_IUcApYh","colab_type":"code","colab":{}},"source":["## PARAMETERS ## \n","\n","clusters_to_compare = [1,4]\n","samples_per_cluster = 5\n","metric = 'net_packetsTx'\n","DATA = perf_data\n","\n","\n","### Fig scaling: \n","num_cl_to_compare = len(clusters_to_compare)\n","\n","\n","\n","####\n","sampled_df = clustering_df[clustering_df['dbscan_clusters'].isin(clusters_to_compare)].groupby('dbscan_clusters').apply(lambda x: x.sample(samples_per_cluster))\n","\n","fig, axes = plt.subplots(nrows = samples_per_cluster, ncols = num_cl_to_compare, figsize = (15*num_cl_to_compare, 5*samples_per_cluster))\n","\n","ix_i = 0\n","\n","for i in clusters_to_compare:\n","    _tmp = list(sampled_df.iloc[sampled_df.index.get_level_values(0).isin([i]),:].index.get_level_values(1))\n","    ix_j = 0\n","    for j in _tmp:\n","        axes[ix_j, ix_i].plot(DATA[j][metric])\n","        axes[ix_j, ix_i].set_ylabel('vmid:'+str(j) + '('+str(i)+')')\n","        axes[ix_j, ix_i].yaxis.set_label_position('right')\n","        ix_j += 1\n","    ix_i += 1"],"execution_count":0,"outputs":[]}]}